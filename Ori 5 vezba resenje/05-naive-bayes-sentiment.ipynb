{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Naivni Bayesov klasifikator spada u algoritme **nagledanog mašinskog učenja**.\n",
    "\n",
    "Naivni Bayes koristi probabilističku metodu, odnosno Bayesovu teoremu, koja opisuje verovatnoću nekog događaja na osnovu drugih događaja koji ga uslovljavaju.\n",
    "\n",
    "Bayesova teorema se matematički iskazuje sledećom formulom:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B | A) \\, P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "gde su:\n",
    "\n",
    "* P(A) i P(B) verovatnoće događaja A i B (**nezavisni jedan od drugog** - ovo je važna pretpostavka Bayesove teoreme)\n",
    "* P(A|B) je uslovna verovatnoća događaja A, ako se desio događaj B\n",
    "* P(B|A) je verovatnoća B, ako se desio A \n",
    "\n",
    "#### Neformalno objašenjenje na jednostavnom primeru\n",
    "\n",
    "Recimo da postoje dva događaja A i B:\n",
    "* **A** - gledao sam film Terminator 2 (i da, to mi je omiljeni film)\n",
    "* **B** - sedeo sam na kauču\n",
    "\n",
    "Koja je verovatnoća da gledam Terminator 2, ako sedim na kauču?\n",
    "\n",
    "![img/t2.jpg](img/t2.jpg)\n",
    "\n",
    "\n",
    "Sad ova 2 događaja treba predstaviti kao Bayesove komponente.\n",
    "\n",
    "Film Terminator 2 sam gledao dosta puta (stvarno) kad sam bio klinac. Bilo je dana kada sam ga gledao i po dva puta dnevno, zaredom. Ali, da uprostimo celu stvar, hajde da kažemo da sam ga gledao 50 puta za godinu dana, što znači:\n",
    "\n",
    "```\n",
    "P(A) = P(gledao sam Terminator 2) = 50 / 365 ~= 0.14, (14% verovatnoća da ću u danu odgledati Terminator 2)\n",
    "```\n",
    "\n",
    "Kada sam kod kuće, uglavnom sedim na kauču (u suprotnom sedim za računarom, gde drugde). Ako radim 8 sati dnevno i spavam 6 sati dnevno, znači da sam 10 sati dnevno kod kuće, od toga recimo 7 sati na kauču, dakle:\n",
    "\n",
    "\n",
    "```\n",
    "P(B) = P(sedeo sam na kauču) = 7 / 24 ~= 0.29, (29% verovatnoća da u toku dana sedim na kauču)\n",
    "```\n",
    "\n",
    "Film Terminator 2 uglavnom sam gledao kod kuće, recimo 95% od svih gledanja, i kada sam ga gledao kod kuće, retko sam ga gledao na računaru, možda 20% od svih gledanja, što dođe:\n",
    "\n",
    "```\n",
    "P(B|A) = P(sedeo sam na kauču dok gledam Terminator 2) = 0.95*(1-0.2) = 0.76\n",
    "```\n",
    "\n",
    "Ok, sada imamo sve što je potrebno da Bayesovom teoremom izračunamo koja je verovatnoća da sam gledao film Terminator 2 ako sedim na kauču:\n",
    "\n",
    "```\n",
    "P(A|B) = P(B|A)*P(A)/P(B) = (0.76 * 0.14) / 0.29 ~= 0.37\n",
    "```\n",
    "\n",
    "Ovo je već zabrinjavajuće... Ispalo je da, ako sam danas sedeo na kauču, postoji čak 37% šanse da sam gledao film Terminator 2. Ipak, ima smisla - dobar je film!\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "### Naivni Bayesov klasifikator\n",
    " \n",
    "Naivni Bayesov klasifikator se veoma često koristi u problemu klasifikacije dokumenata. Naivni Bayes je dobar izbor za ovo jer je veoma brz i dobro se nosi sa velikim brojem osobina podataka (npr. reči u dokumentu), i zapravo je veoma efektivan. Naivni Bayes inače važi za jedne od najjednostavnijih pristupa, koji uprkos tome daje odlične rezultate.\n",
    "\n",
    "---\n",
    "\n",
    "### Naivni Bayes kao klasifikator sentimenta teksta\n",
    "\n",
    "Sentiment analiza teksta predstavlja identifikaciju i ekstraktovanje subjektivnih informacija iz teksta. Najjednostavniji primer sentiment analize je procena da li je neki teskt napisan u pozitivnom ili negativnom kontekstu. Ovo je trenutno veoma aktuelna oblast istraživanja zbog eksplozije korišćenja socijalnih mreža - npr. nakon objavljivanja neke vesti ili statusa vezano za neki proizvod, veliki brendovi često žele da znaju kako su korisnici to prihvatili (bez \"ručnog\" analiziranja komentar po komentar).\n",
    "\n",
    "Na ovim vežbama, koristićemo Naivni Bayesov klasifikator za klasifikaciju sentimenta korisničkih **realnih** recenzija filmova i knjiga, ali su ove recenzije skupljane na društvenim mrežama (što se oslikava malo \"slobodnijim\" rečnikom). Dataset za ovaj se nalazi u ```train.tsv```, gde prva kolona predstavlja sentiment recenzije (**pos** = pozitivna recenzija, **neg** = negativna recenzija), a druga kolona tekst same recenzije.\n",
    "\n",
    "**Primeri pozitivnih recenzija:**\n",
    "* Hey I loved The Da Vinci Code!..\n",
    "* mission impossible 2 rocks!!....\n",
    "* lol I love Harry Potter like a fat kid loves cake.\n",
    "\n",
    "**Primeri negativnih recenzija:**\n",
    "* Brokeback Mountain was depressing!\n",
    "* I hate Harry Potter even more now. >:\n",
    "* \"Christmas Mission's Trip with our very own Tom Cruise: \"\" Mission Impossible is for stupid people \"\"..\"\n",
    "\n",
    "Korišćenjem Naivnog Bayesa, problem klasifikator sentimenta teksta možemo definisati kao:\n",
    "\n",
    "```\n",
    "P(S) = P(recenzija je određenog sentimenta), P(s1) za pozitivnu, P(s2) za negativnu\n",
    "P(T) = P(napisan određeni niz reči koji predstavlja recenziju)\n",
    "P(T|S) = P(određen niz reči predstavlja recenziju određenog sentimenta)\n",
    "P(S|T) = ovo je ono što treba izračunati -> za dati niz reči koji predstavlja recenziju izračunati verovatnoću da je ta recenzija određenog sentimenta\n",
    "\n",
    "... naravno, isto ovako za negativne recenzije.\n",
    "```\n",
    "\n",
    "Obratite pažnju da je u prethodnim izrazima tekst (recenzija) predstavljan kao \"niz reči\", stoga ćemo primenjivati verziju Naivnog Bayesa prilagođenu ovom problemu.\n",
    "\n",
    "$$\n",
    "P(s_i|T) = \\frac{P(T | s_i) \\, P(s_i)}{P(T)}\n",
    "$$\n",
    "\n",
    "ako tekst T razložimo na reči $ t \\in T $ od kojih se sastoji, problem transformišemo u:\n",
    "\n",
    "$$\n",
    "P(s_i|T) = \\prod_{t \\in T}\\frac{P(t| s_i)}{P(t)} \\, P(s_i)\n",
    "$$\n",
    "\n",
    "Pošto su $ \\frac{P(t| s_i)}{P(t)} $ brojevi manji od 1, računanje više ovakvih brojeva može prouzrokovati problem numeričke prirode - *floating point undeflow*. Stoga ćemo iskoristiti jedan \"trik\" tako što ćemo izraz logaritmovati (i samim tim proizvod pretvoriti u sumu).\n",
    "\n",
    "$$\n",
    "ln(P(s_i|T)) = \\sum_{t \\in T}ln(\\frac{P(t| s_i)}{P(t)}) \\, + ln(P(s_i))\n",
    "$$\n",
    "\n",
    "Zatim, vratimo ovo sve u originalni oblik:\n",
    "\n",
    "$$\n",
    "P(s_i|T) = e^{\\sum_{t \\in T}ln(\\frac{P(t| s_i)}{P(t)}) \\, + ln(P(s_i))}\n",
    "$$\n",
    "\n",
    "\n",
    "**Važna napomena:** Postupak se naziva \"naivni\" jer se pretpostavlja da su stavke u formuli međusobno nezavisne. U našem slučaju, realno je očekivati da reči u tekstu nisu potpuno nezavisne, jer reči zajedno daju značenje celoj rečenici. Dakle, mi suštinski \"kršimo\" osnovnu pretpostavku ovog algoritma, ali videćete da će svejedno davati zadovoljavajuće rezultate.\n",
    "\n",
    "---\n",
    "\n",
    "Pošto ćemo raditi sa slobodnim tekstom (prirodnim jezikom) i rečima iz teksta, neophodno je uraditi nekoliko stvari kako bi se tekst uprostio i olakšao za dalju upotrebu. Neke od tehnika preprocesiranja teksta su:\n",
    "\n",
    "* Izbacivanje tzv. **STOP** reči - to su reči koje se često pojavljuju i ne nose neko relevantno značenje za samu klasifikaciju teksta. Npr. za engleski jezik te reči su: ```a, an, the, and, of, for, ...```\n",
    "* Izbacivanje znakova interpunkcije\n",
    "* Stemizacija reči - svođenje reči na korenski oblik. Npr. ```running -> run, computers -> computer, glasses -> glass, ...```\n",
    "* Svođenje svih reči na mala slova - lowercase\n",
    "* itd.\n",
    "\n",
    "Ovo preprocesiranje teksta često predstavlja veoma važan korak u implemetaciji dobrih klasifikatora, i značajno utiče na performanse klasifikatora.\n",
    "\n",
    "---\n",
    "\n",
    "### Zadaci\n",
    "\n",
    "**TODO 1:** Učitati recenzije i sentimente u metodi ```load_data()```, iz datoteke ```data/train.tsv```. Rezultat metode treba da budu dve liste: ```texts``` i ```sentiments```.\n",
    "\n",
    "**TODO 2:** Implementirati jednostavno preprocesiranje teksta (metoda ```preprocess(text)```):\n",
    "\n",
    "* izbacivanje znakova interpunkcije\n",
    "* svođenje celog teksta na mala slova\n",
    "\n",
    "**TODO 3:** Implementirati tokenizaciju teksta na reči (metoda ```tokenize(text)```). Rezultat treba da bude lista reči koje se pojavljuju u datom tekstu.\n",
    "\n",
    "**TODO 4:** Implementirati prebrojavanje reči u datum tekstu (metoda ```count_words(text)```). Rezultat treba da bude mapa, čiji ključevi su reči, a vrednosti broj ponavljanja te reči u datom tekstu.\n",
    "\n",
    "**TODO 5:** Napuniti *bag-of-words* strukture. *Bag-of-words* je struktura koja za ceo korpus tekstova daje mapu, čiji ključevi su reči, a vrednosti broj ponavljanja te reči u celom korpusu tekstova.\n",
    "\n",
    "**TODO 6:** Implementirati Naivni Bayesov klasifikator za sentiment teksta (recenzije). Rezultat treba da bude mapa verovatnoća koja opisuje u kojoj meri je dati tekst (recenzija) klasifikovan kao pozitivan i negativan.\n",
    "\n",
    "\n",
    "```\n",
    "P(bilo koja recenzija pozitivna) = ukupan broj pozitivnih recenzija / ukupan broj svih recenzija\n",
    "P(bilo koja recenzija negativna) = ukupan broj pozitivnih negativna / ukupan broj svih recenzija\n",
    "\n",
    "P(reč se pojavljuje u recenziji) = broj ponavljanja ove reči u celom korpusu / ukupan broj reči u celom korpusu\n",
    "\n",
    "P(reč se pojavljuje u poz. recenziji) = (broj ponavljanja ove reči u poz. korpusu + 1) / (ukupan broj reči u poz. korpusu + ukupan broj reči u rečniku)\n",
    "P(reč se pojavljuje u neg. recenziji) = (broj ponavljanja ove reči u neg. korpusu + 1) / (ukupan broj reči u neg. korpusu + ukupan broj reči u rečniku)\n",
    "```\n",
    "\n",
    "\n",
    "**TODO (domaći):** Implementirati jednostavnu stemizaciju reči i izbacivanje osnovnih STOP reči u koraku preprocesiranja teksta.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Zaključak\n",
    "\n",
    "* Naivni Bayesov klasifikator je izuzetno jednostavna probabilistička metoda\n",
    "* Iako jednostavan, veoma efikasan - sve se svodi samo na računanje verovatnoća\n",
    "* Iako se često krši osnovna pretpostavka postupka (da su promenljive međusobno nezavisne), i dalje se dobijaju dobri rezultati\n",
    "* Naivni Bayes se ne koristi samo za klasifikaciju teksta, već naprotiv, i za mnoge druge potrebe: klasifikaciju (u opštem smislu), klasifikaciju slika, dijagnozu bolesti na osnovu simptoma, itd...\n",
    "\n",
    "\n",
    "#### Prednosti\n",
    "\n",
    "* Brzo \"obučavanje\" - jednom se prođe kroz podatke, tzv. *single-scan*\n",
    "* Brza klasifikacija\n",
    "* Nije osetljiv na šum i nerelevantne osobine\n",
    "* Može da koristi i realne i diskretne podatke\n",
    "\n",
    "#### Mane\n",
    "\n",
    "* Pretpostavlja da su osobine podataka međusobno nezavisne (veoma često netačno)\n",
    "\n",
    "---\n",
    "\n",
    "**DODATNO:**\n",
    "\n",
    "* Primeniti Naivni Bayes za implementaciju klasifikatora novinskih članaka na 20 grupa, data set dostupan na http://qwone.com/~jason/20Newsgroups/. (1 boda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}